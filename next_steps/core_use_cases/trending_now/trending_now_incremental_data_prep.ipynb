{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f018592",
   "metadata": {},
   "source": [
    "# Amazon Personalize introduces - Trending Now\n",
    "\n",
    "This notebook will help you prepare incremental interactions dataset to use it with Trending Now recipe in [Amazon Personalize]\n",
    "\n",
    "User interests can change based on a variety of factors, such as external events or the interests of other users. It is critical for websites and apps to tailor their recommendations to these changing interests to improve user engagement. With Trending-Now, you can surface items from your catalogue that are rising in popularity faster with higher velocity than other items, such as trending news, popular social content or newly released movies. Amazon Personalize looks for items that are rising in popularity at a faster rate than other catalogue items to help users discover items that are engaging their peers. Amazon Personalize also allows customers to define the time periods over which trends are calculated depending on their unique business context, with options for every 30 mins, 1 hour, 3 hours or 1 day, based on the most recent interactions data from users. This notebook will demonstrate how the new recipe aws-trending-now (or aws-vod-trending-now for recommenders) can help recommend the top trending items from the interactions dataset.\n",
    "\n",
    "The estimated time to run through this notebook is about 10 minutes. \n",
    "\n",
    "## How to use the Notebook\n",
    "\n",
    "The code is broken up into cells like the one below. There's a triangular Run button at the top of this page that you can click to execute each cell and move onto the next, or you can press `Shift` + `Enter` while in the cell to execute it and move onto the next one.\n",
    "\n",
    "As a cell is executing you'll notice a line to the side showcase an `*` while the cell is running or it will update to a number to indicate the last cell that completed executing after it has finished exectuting all the code within a cell.\n",
    "\n",
    "Simply follow the instructions below and execute the cells to get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ca987",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Python ships with a broad collection of libraries and we need to import those as well as the ones installed to help us like [boto3](https://aws.amazon.com/sdk-for-python/) (AWS SDK for python) and [Pandas](https://pandas.pydata.org/)/[Numpy](https://numpy.org/) which are core data science tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca0fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import boto3\n",
    "import json as json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c51d036",
   "metadata": {},
   "source": [
    "### Prepare latest interactions\n",
    "Let us create some dummy latest interaction events with some random user_ids and item_ids to import it to the Amaon Personalize using incremental dataset import job. We will use the interaction timestamp to be of the current timestamp to replicate latest interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf9c67",
   "metadata": {},
   "source": [
    "Below are few randomly selected USER_ID’s that are used for generating incremental interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_list = ['20371','63409','54535','119138','58953','82982','19044','139171','98598','23822','112012','121380','2660','46948','5656','68919','152414','31234','88240','40395','49296','80280','150179','138474','124489','145218','141810','82607']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6866dc",
   "metadata": {},
   "source": [
    "Below are few randomly selected ITEM_ID’s that are used for generating incremental interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f1a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_list = [ '153','2459','1792','3948','2363','260','61248','6539','2407','8961']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d781b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_epoch = int(time.time())\n",
    "time_epoch = time_epoch-3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bbf090",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_df = pd.DataFrame(columns=[\"USER_ID\",\"ITEM_ID\",\"TIMESTAMP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65843073",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for j in range(0,10):\n",
    "    for k in users_list:\n",
    "        for l in items_list:\n",
    "            time_epoch = time_epoch+1\n",
    "            list_row = [str(k),str(l),time_epoch]\n",
    "            inc_df.loc[i] = list_row\n",
    "            i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdea277",
   "metadata": {},
   "source": [
    "In the cell below, we will write our cleaned data to a file named \"interactions_incremental_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ea757",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_file_path = 'interactions_incremental_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff2dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_df.to_csv(incremental_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f079d99f",
   "metadata": {},
   "source": [
    "File named 'interactions_incremental_data.csv' is created in this notebook instance.\n",
    "\n",
    "Download the file into your local machine and upload it to an S3 bucket before you import the data it into the Amazon Personalize."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
